# AI4S-PPFL: Privacy-Preserving Federated Learning for Science

Advancing artificial intelligence (AI) and machine learning (ML) is crucial for transforming scientific processes. **AI4S-PPFL** is a multi-institutional research project focused on developing cutting-edge **privacy-preserving federated learning (PPFL)** algorithms and frameworks to enable the training of large foundation models on distributed, sensitive datasets.

### Project Objectives

The AI4S-PPFL project addresses critical challenges in:
- **Efficiency**: Optimizing communication, memory, and energy usage in federated learning.
- **Privacy and Fairness**: Ensuring robust data privacy and equitable participation across collaborators.
- **Synthetic Data Integration**: Generating high-quality synthetic datasets to enhance privacy without sacrificing model utility.

By tackling these challenges, AI4S-PPFL aims to ensure sustainable, trustworthy deployment of foundation models in science.

## Collaborating Institutions and Key Team Members 

### Argonne National Laboratory 
- Kibaek Kim (Lead PI)
- Ravi Madduri (Co-PI)
- Todd Munson (Co-PI)
- Krishnan Raghavan (Co-PI)
- Rob Ross (Co-PI)
- Matthieu Dorier (Co-PI)

### Brookhaven National Laboratory
- Thomas Flynn (PI)
- Ai Kagawa (Co-PI)
- Byung-Jun Yoon (Co-PI)

### Oak Ridge National Laboratory
- Olivera Kotevska (PI)
- Christian Engelmann (Co-PI)

### Arizona State University
- Minseok Ryu (PI)

### Rutgers University
- Farzad Yousefian (PI)

## Acknowledgement

This project is supported by the U.S. Department of Energy, Office of Science, Advanced Scientific Computing Research, under Contract DE-AC02-06CH11357.
